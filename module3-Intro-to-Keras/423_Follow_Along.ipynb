{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')\n",
    "X = df[['x1', 'x2']].values\n",
    "y = df['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4 samples\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 0s 77ms/sample - loss: 0.7195 - accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.7191 - accuracy: 0.7500\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.7188 - accuracy: 0.7500\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.7184 - accuracy: 0.7500\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 749us/sample - loss: 0.7180 - accuracy: 0.7500\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 748us/sample - loss: 0.7177 - accuracy: 0.7500\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 499us/sample - loss: 0.7173 - accuracy: 0.7500\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 496us/sample - loss: 0.7170 - accuracy: 0.7500\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 749us/sample - loss: 0.7166 - accuracy: 0.7500\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 995us/sample - loss: 0.7163 - accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b3c12c5860>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# This is our perceptron from Monday's by-hand: \n",
    "model = Sequential()\n",
    "model.add(Dense(1,input_dim=2, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X,y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 14ms/sample - loss: 0.7159 - accuracy: 0.7500\n",
      "accuracy: 75.0\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, y)\n",
    "print(f\"{model.metrics_names[1]}: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Stretch - use dropout \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X Variable Types\n",
    "X_train = X_train.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') /255.\n",
    "\n",
    "# Correct Encoding on Y\n",
    "# What softmax expects = [0,0,0,0,0,1,0,0,0,0]\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(812)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, input_dim=784, activation=\"softmax\"))\n",
    "\n",
    "# number of neurons in the layer, number of input items, activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow Along\n",
    "In the Sequential api model, you specify a model architecture by 'sequentially specifying layers. This type of specification works well for feed forward neural networks in which the data flows in one direction (forward propagation) and the error flows in the opposite direction (backwards propagation). The Keras Sequential API follows a standardarized worklow to estimate a 'net:\n",
    "\n",
    "-Load Data\n",
    "\n",
    "-Define Model\n",
    "\n",
    "Compile Model\n",
    "\n",
    "Fit Model\n",
    "\n",
    "Evaluate Model\n",
    "\n",
    "You saw these steps in our Keras Perceptron Sample, but let's walk thru each step in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a \"Dense\" layer to our model is how we add \"vanilla\" perceptron-based layers to our neural network. These are also called \"fully-connected\" or \"densely-connected\" layers. They're used as a layer type in lots of other Neural Net Architectures but they're not referred to as perceptrons or multi-layer perceptrons very often in those situations even though that's what they are.\n",
    "\n",
    "\"Just your regular densely-connected NN layer.\"\n",
    "\n",
    "The first argument is how many neurons we want to have in that layer. To create a perceptron model we will just set it to 1. We will tell it that there will be 8 inputs coming into this layer from our dataset and set it to use the sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model\n",
    "Using binary_crossentropy as the loss function here is just telling keras that I'm doing binary classification so that it can use the appropriate loss function accordingly. If we were predicting non-binary categories we might assign something like categorical_crossentropy. We're also telling keras that we want it to report model accuracy as our main error metric for each epoch. We will also be able to see the overall accuracy once the model has finished training.\n",
    "\n",
    "### Adam Optimizer\n",
    "Check out this links for more background on the Adam optimizer and Stohastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.4679 - accuracy: 0.8773\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.3039 - accuracy: 0.9158\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.2835 - accuracy: 0.9207\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.2734 - accuracy: 0.9240\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.2667 - accuracy: 0.9258\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.2624 - accuracy: 0.9272\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 2s 34us/sample - loss: 0.2582 - accuracy: 0.9283\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.2552 - accuracy: 0.9294\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 2s 33us/sample - loss: 0.2526 - accuracy: 0.9299\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 32us/sample - loss: 0.2511 - accuracy: 0.9305\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b3c86e7128>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.2650 - accuracy: 0.9266\n",
      "/n\n",
      "loss: 0.7159004211425781\n",
      "accuracy: 75.0\n"
     ]
    }
   ],
   "source": [
    "scpres = model.evaluate(X_test, y_test)\n",
    "print(\"/n\")\n",
    "print(f\"{model.metrics_names[0]}: {scores[0]}\")\n",
    "print(f\"{model.metrics_names[1]}: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TENSORBOARD_BINARY=C:\\\\Users\\\\ajenk\\\\Anaconda3\\\\envs\\\\U4-S2-NN\\\\Scripts\\\\tensorboard\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PATH = os.getenv('PATH')\n",
    "%env TENSORBOARD_BINARY=C:\\\\Users\\\\ajenk\\\\Anaconda3\\\\envs\\\\U4-S2-NN\\\\Scripts\\\\tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "60000/60000 [==============================] - 8s 128us/sample - loss: 0.4712 - accuracy: 0.8779 - val_loss: 0.3101 - val_accuracy: 0.9147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b3c9d01e10>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(10, input_dim = 784, activation='softmax')\n",
    "    \n",
    "])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x=X_train,\n",
    "         y=y_train,\n",
    "         epochs=1,\n",
    "         validation_data=(X_test, y_test),\n",
    "         callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always save data and log files! This is what lets you build on your experimentations with architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 7980), started 0:01:17 ago. (Use '!kill 7980' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-daa427962e7ed1f2\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-daa427962e7ed1f2\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajenk\\GitHub\\DS-Unit-4-Sprint-2-Neural-Networks\\module3-Intro-to-Keras\n"
     ]
    }
   ],
   "source": [
    "!cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard gives you tons of ways to see what's going"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "    Flatten(),\n",
    "    Dense(128, input_dim=784, activation='relu'),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 5s 86us/sample - loss: 9.2762 - accuracy: 0.1557 - val_loss: 9.7934 - val_accuracy: 0.1257\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 5s 81us/sample - loss: 9.7816 - accuracy: 0.1281 - val_loss: 9.1604 - val_accuracy: 0.1294\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 9.4820 - accuracy: 0.1394 - val_loss: 10.6041 - val_accuracy: 0.1394\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 10.4706 - accuracy: 0.1433 - val_loss: 10.6041 - val_accuracy: 0.1394\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 5s 83us/sample - loss: 10.4706 - accuracy: 0.1433 - val_loss: 10.6041 - val_accuracy: 0.1394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24323066f60>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x=X_train,\n",
    "         y=y_train,\n",
    "         epochs=5,\n",
    "         validation_data=(X_test, y_test),\n",
    "         callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Additional Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(),\n",
    "    Dense(128, input_dim=784, activation='relu'),\n",
    "    Dense(128, input_dim=784, activation='relu'),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 7s 120us/sample - loss: 0.2295 - accuracy: 0.9317 - val_loss: 0.1268 - val_accuracy: 0.9601\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 6s 98us/sample - loss: 0.0971 - accuracy: 0.9697 - val_loss: 0.0934 - val_accuracy: 0.9722\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 6s 105us/sample - loss: 0.0685 - accuracy: 0.9783 - val_loss: 0.0899 - val_accuracy: 0.9736\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 126us/sample - loss: 0.0522 - accuracy: 0.9835 - val_loss: 0.0989 - val_accuracy: 0.9708\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 6s 108us/sample - loss: 0.0406 - accuracy: 0.9864 - val_loss: 0.0727 - val_accuracy: 0.9776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x243241b2f98>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Flatten(),\n",
    "    Dense(128, input_dim=784, activation='relu'),\n",
    "    Dense(128, input_dim=784, activation='relu'),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x=X_train,\n",
    "         y=y_train,\n",
    "         epochs=5,\n",
    "         validation_data=(X_test, y_test),\n",
    "         callbacks=[tensorboard_callback])\n",
    "\n",
    "# we should redefine log dir every time we call a new model -\n",
    "# will create a different log file with a different datetime name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow - Trastically better validation accuracies. It looks like there are patterns that it takes a given layer to achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different optimization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 11s 183us/sample - loss: 0.2281 - accuracy: 0.9319 - val_loss: 0.1296 - val_accuracy: 0.9613\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0945 - accuracy: 0.9710 - val_loss: 0.0805 - val_accuracy: 0.9749\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 8s 134us/sample - loss: 0.0660 - accuracy: 0.9791 - val_loss: 0.0911 - val_accuracy: 0.9711\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 8s 138us/sample - loss: 0.0512 - accuracy: 0.9832 - val_loss: 0.0806 - val_accuracy: 0.9762\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 8s 130us/sample - loss: 0.0407 - accuracy: 0.9869 - val_loss: 0.0756 - val_accuracy: 0.9776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2432530efd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Flatten(),\n",
    "    Dense(128, input_dim=784, activation='relu'),\n",
    "    Dense(128, input_dim=784, activation='relu'),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x=X_train,\n",
    "         y=y_train,\n",
    "         epochs=5,\n",
    "         validation_data=(X_test, y_test),\n",
    "         callbacks=[tensorboard_callback])\n",
    "\n",
    "# we should redefine log dir every time we call a new model -\n",
    "# will create a different log file with a different datetime name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.2330 - accuracy: 0.9293 - val_loss: 0.1086 - val_accuracy: 0.9654\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 6s 103us/sample - loss: 0.0997 - accuracy: 0.9696 - val_loss: 0.0852 - val_accuracy: 0.9718\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.0709 - accuracy: 0.9781 - val_loss: 0.0844 - val_accuracy: 0.9740\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.0561 - accuracy: 0.9819 - val_loss: 0.0842 - val_accuracy: 0.9737\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.0456 - accuracy: 0.9854 - val_loss: 0.0831 - val_accuracy: 0.9762\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.0368 - accuracy: 0.9880 - val_loss: 0.0848 - val_accuracy: 0.9766\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.0317 - accuracy: 0.9898 - val_loss: 0.1010 - val_accuracy: 0.9735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24326caad68>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = EarlyStopping(monitor='accuracy', min_delta=.01, patience=3)\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(),\n",
    "    Dense(128, input_dim=784, activation='relu'),\n",
    "    Dense(128, input_dim=784, activation='relu'),\n",
    "    Dense(128, input_dim=784, activation='relu'),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x=X_train,\n",
    "         y=y_train,\n",
    "         epochs=50,\n",
    "         validation_data=(X_test, y_test),\n",
    "         callbacks=[tensorboard_callback, stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S2-NN (Python3)",
   "language": "python",
   "name": "u4-s2-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
